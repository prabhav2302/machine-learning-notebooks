{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "# Load dataset to some variable \n",
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "#  Create X and Y variables - X holding the .data and Y holding .target \n",
    "X = boston_data.data\n",
    "y = boston_data.target\n",
    "\n",
    "#  Reshape Y to be a rank 2 matrix using y.reshape()\n",
    "y=y.reshape(X.shape[0],1)\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "print('The number of features is: ', X.shape[1])\n",
    "\n",
    "# Printing out the features\n",
    "print('The features: ', boston_data.feature_names)\n",
    "\n",
    "# The number of examples\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "\n",
    "# Observing the first 2 rows of the data\n",
    "print(X[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. Using PolynomialFeatures(degree=2)\n",
    "# Transform X and save it into X_2 using poly.fit_transform(X)\n",
    "# Simply copy Y into Y_2 \n",
    "\n",
    "pol = PolynomialFeatures(degree=2)\n",
    "X_2 = pol.fit_transform(X)\n",
    "y_2 = y\n",
    "\n",
    "#Here I have created polynomial features which will be implemented later on in the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(y_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_coeff_ridge_normaleq function returns the optimum weight values when the ridge term is also taken into\n",
    "account whilst minimizing the least-squared loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    " # use np.linalg.pinv(...)\n",
    "    # W = ((X_T . X + alpha * I )^-1) . X_T . y\n",
    "    bias_column = np.ones((X_train.shape[0],1))\n",
    "    X_train = np.c_[X_train,bias_column]\n",
    "    X_T = X_train.transpose()\n",
    "    X_T_X = X_T.dot(X_train)\n",
    "\n",
    "    m_1 = X_T_X + alpha * np.identity (X_T_X.shape[1])\n",
    "    m_1 = np.linalg.pinv(m_1)\n",
    "    m_2 = X_T.dot(y_train)\n",
    "    w = m_1.dot(m_2)\n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_coeff_linear_normaleq returns the optimum weight values when least squares is used as loss function. \n",
    "Here, there is no ridge term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define get_coeff_linear_normaleq function. Use the normal equation method.\n",
    "# Return w values\n",
    "\n",
    "def get_coeff_linear_normaleq(X_train, y_train):\n",
    "    # use np.linalg.pinv(...)\n",
    "    bias_column = np.ones((X_train.shape[0],1))\n",
    "    X_train = np.c_[X_train,bias_column]\n",
    "    X_T = X_train.transpose()\n",
    "    X_T_X = X_T.dot(X_train)\n",
    "    m_1 = np.linalg.pinv(X_T_X)\n",
    "    m_2 = X_T.dot(y_train)\n",
    "    w = m_1.dot(m_2)\n",
    "\n",
    "    return w\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluate_err function returns the errors in our models predictions of the training and test test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluate_err_ridge function.\n",
    "# Return the train_error and test_error values\n",
    "\n",
    "\n",
    "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
    "    #pred_train=prediction using w and X_tran+np.mean(y_train)\n",
    "    #pred_test=prediction using w and X_test\n",
    "    #remember to add the mean back\n",
    "    bias_column = np.ones((X_train.shape[0],1))\n",
    "    X_train = np.c_[X_train,bias_column]\n",
    "    pred_train = X_train.dot(w)\n",
    "    test_bias_column = np.ones((X_test.shape[0],1))\n",
    "    X_test = np.c_[X_test,test_bias_column]  \n",
    "    pred_test = X_test.dot(w) \n",
    "    total_train_error = np.square(np.subtract(y_train,pred_train))\n",
    "    train_error = total_train_error.mean()  #mean squared train error\n",
    "    total_test_error = np.square(np.subtract(y_test,pred_test))\n",
    "    test_error = total_test_error.mean() #mean squared test error\n",
    "    \n",
    "    return train_error, test_error\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The k_fold_cross_validation function randomly divides the set of observations into k groups, or folds.\n",
    "The first fold is treated as a test set, and the method is fit on the remaining k âˆ’ 1 folds.\n",
    "The process is then repeated until each of the remaining k-1 folds has been used as a test set. Then the \n",
    "average error from the k-fold cross validation is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish writting the k_fold_cross_validation function. \n",
    "# Returns the average training error and average test error from the k-fold cross validation\n",
    "# Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha,regression_type):\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_test_error = 0\n",
    "    total_train_error = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "        # Subtract y_train_mean from y_train and y_test\n",
    "        y_train_mean = np.mean(y_train)\n",
    "        y_train = y_train - y_train_mean\n",
    "        y_test = y_test - y_train_mean\n",
    "        \n",
    "        # Scaling the data matrix\n",
    "        # Using scaler=preprocessing.StandardScaler().fit(...)\n",
    "        # And scaler.transform(...)\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Determine the training error and the test error\n",
    "        # Use get_coeff_linear_normaleq or get_coeff_ridge_normaleq to get w\n",
    "        # And use evaluate_err()\n",
    "\n",
    "        if regression_type == \"linear\":\n",
    "            w_linear_matrix = get_coeff_linear_normaleq(X_train, y_train)\n",
    "            current_train_error, current_test_error = evaluate_err(X_train,X_test, y_train, y_test ,w_linear_matrix)\n",
    "            total_train_error = total_train_error+current_train_error\n",
    "            total_test_error = total_test_error+current_test_error\n",
    "\n",
    "\n",
    "        else:\n",
    "            w_ridge = get_coeff_ridge_normaleq( X_train, y_train , alpha)\n",
    "            current_train_error, current_test_error = evaluate_err(X_train,X_test,y_train,y_test,w_ridge)\n",
    "            total_train_error = total_train_error+current_train_error\n",
    "            total_test_error = total_test_error+ current_test_error\n",
    "\n",
    "       ##############\n",
    "    \n",
    "    #compute average for all Ks\n",
    "    avg_train_error = total_train_error/k\n",
    "    avg_test_error = total_test_error/k\n",
    "\n",
    "        \n",
    "        \n",
    "    return  avg_train_error,avg_test_error\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can compare the linear model with the ridge model we must find the optimum alpha for the ridge model. \n",
    "Therefore before I compare the results of the linear and ridge model I have first found the optimum alpha for the ridge model in both cases (when the dataset has no polynomial features and when it does)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Alpha= 10.0--------------------\n",
      "Train Error: 21.89290115757019 Test Error: 23.68858300163877\n",
      "--------------------Alpha= 31.622776601683793--------------------\n",
      "Train Error: 22.2854440556975 Test Error: 24.01784029738615\n",
      "--------------------Alpha= 100.0--------------------\n",
      "Train Error: 23.72548838488208 Test Error: 25.293852553695142\n",
      "--------------------Alpha= 316.22776601683796--------------------\n",
      "Train Error: 28.16655409854012 Test Error: 29.457296222896787\n",
      "--------------------Alpha= 1000.0--------------------\n",
      "Train Error: 38.53214872734357 Test Error: 39.48949335939869\n",
      "--------------------Alpha= 3162.2776601683795--------------------\n",
      "Train Error: 54.00702649955449 Test Error: 54.64719107794949\n",
      "--------------------Alpha= 10000.0--------------------\n",
      "Train Error: 69.2597817132532 Test Error: 69.71851750884511\n",
      "--------------------Alpha= 31622.776601683792--------------------\n",
      "Train Error: 78.53074243629268 Test Error: 78.92162110435102\n",
      "--------------------Alpha= 100000.0--------------------\n",
      "Train Error: 82.40325012491367 Test Error: 82.7724042894599\n",
      "--------------------Alpha= 316227.7660168379--------------------\n",
      "Train Error: 83.75514887733652 Test Error: 84.11748625821778\n",
      "--------------------Alpha= 1000000.0--------------------\n",
      "Train Error: 84.19680334862025 Test Error: 84.5569941832673\n",
      "--------------------Alpha= 3162277.6601683795--------------------\n",
      "Train Error: 84.33793107733854 Test Error: 84.69744416333866\n",
      "--------------------Alpha= 10000000.0--------------------\n",
      "Train Error: 84.38270764054002 Test Error: 84.74200651304514\n"
     ]
    }
   ],
   "source": [
    "# determining best alpha for ridge model when dataset has no polynomial features:\n",
    "alpha_values = np.logspace(1, 7, num=13)\n",
    "for i in alpha_values:\n",
    "    avg_ridge_train_error, avg_ridge_test_error, = k_fold_cross_validation(10,X,y,i,\"ridge\")\n",
    "    print(\"-\"*20 +\"Alpha= \"+str(i)+\"-\"*20)\n",
    "    print(\"Train Error: \"+str(avg_ridge_train_error)+\" Test Error: \"+str(avg_ridge_test_error))\n",
    "\n",
    "#from here we can see that the alpha was gives the minimum test error is alpha = 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Alpha= 10.0--------------------\n",
      "Train Error: 10.049055874118885 Test Error: 13.476138001136226\n",
      "--------------------Alpha= 31.622776601683793--------------------\n",
      "Train Error: 12.751706269046771 Test Error: 15.829601969051158\n",
      "--------------------Alpha= 100.0--------------------\n",
      "Train Error: 16.222690593210803 Test Error: 18.98001881500993\n",
      "--------------------Alpha= 316.22776601683796--------------------\n",
      "Train Error: 19.700253646409855 Test Error: 22.068692308062744\n",
      "--------------------Alpha= 1000.0--------------------\n",
      "Train Error: 24.287457983108894 Test Error: 26.21847559440484\n",
      "--------------------Alpha= 3162.2776601683795--------------------\n",
      "Train Error: 33.08666831985054 Test Error: 34.58026601880018\n",
      "--------------------Alpha= 10000.0--------------------\n",
      "Train Error: 46.76199935941409 Test Error: 47.78952058756748\n",
      "--------------------Alpha= 31622.776601683792--------------------\n",
      "Train Error: 62.009001916836056 Test Error: 62.646288813443356\n",
      "--------------------Alpha= 100000.0--------------------\n",
      "Train Error: 74.28758894586744 Test Error: 74.73795160641836\n",
      "--------------------Alpha= 316227.7660168379--------------------\n",
      "Train Error: 80.69255135236737 Test Error: 81.07998618951191\n",
      "--------------------Alpha= 1000000.0--------------------\n",
      "Train Error: 83.16723569778432 Test Error: 83.53524235105219\n",
      "--------------------Alpha= 3162277.6601683795--------------------\n",
      "Train Error: 84.00579580096273 Test Error: 84.36776579104057\n",
      "--------------------Alpha= 10000000.0--------------------\n",
      "Train Error: 84.2770062647736 Test Error: 84.63708052069056\n"
     ]
    }
   ],
   "source": [
    "# determining best alpha for ridge model when dataset has polynomial features:\n",
    "alpha_values = np.logspace(1, 7, num=13)\n",
    "for i in alpha_values:\n",
    "    avg_ridge_train_error, avg_ridge_test_error, = k_fold_cross_validation(10,X_2,y_2,i,\"ridge\")\n",
    "    print(\"-\"*20 +\"Alpha= \"+str(i)+\"-\"*20)\n",
    "    print(\"Train Error: \"+str(avg_ridge_train_error)+\" Test Error: \"+str(avg_ridge_test_error))\n",
    "\n",
    "#from here we can see that the alpha was gives the minimum test error is alpha = 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated the optimum alpha for both the ridge models we can go ahead and compare the ridge models \n",
    "with the linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are comparing the performance of the linear and ridge models when the dataset has <b>no polynomial features</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Linear Train Error with no polynomial features:  21.806183575851065\n",
      "Average Linear Test Error with no polynomial features:  23.636068605428157\n",
      "Average Ridge Train Error with no polynomial features:  21.89290115757019\n",
      "Average Ridge Test Error with no polynomial features:  23.68858300163877\n"
     ]
    }
   ],
   "source": [
    "# print the error for the both linear regression and ridge regression when dataset has no polynomial features\n",
    "# the error should include both training error and testing error\n",
    "avg_linear_train_error_nopoly, avg_linear_test_error_nopoly = k_fold_cross_validation(10,X,y,0,\"linear\")\n",
    "print(\"Average Linear Train Error with no polynomial features: \", avg_linear_train_error_nopoly)\n",
    "print(\"Average Linear Test Error with no polynomial features: \",avg_linear_test_error_nopoly)\n",
    "\n",
    "avg_ridge_train_error_nopoly, avg_ridge_test_error_nopoly = k_fold_cross_validation(10,X,y,10,\"ridge\")\n",
    "print(\"Average Ridge Train Error with no polynomial features: \", avg_ridge_train_error_nopoly)\n",
    "print(\"Average Ridge Test Error with no polynomial features: \",avg_ridge_test_error_nopoly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are comparing the performance of the linear and ridge models when the dataset <b> has polynomial features</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Linear Train Error with polynomial features:  5.8088208160124655\n",
      "Average Linear Test Error with polynomial features:  11.854968236414866\n",
      "Average Ridge Train Error with polynomial features:  10.049055874118885\n",
      "Average Ridge Test Error with polynomial features:  13.476138001136226\n"
     ]
    }
   ],
   "source": [
    "# print the error for the both linear regression and ridge regression when dataset has polynomial features\n",
    "# the error should include both training error and testing error\n",
    "avg_linear_train_error_poly, avg_linear_test_error_poly, = k_fold_cross_validation(10,X_2,y_2,0,\"linear\")\n",
    "print(\"Average Linear Train Error with polynomial features: \",avg_linear_train_error_poly)\n",
    "print(\"Average Linear Test Error with polynomial features: \",avg_linear_test_error_poly)\n",
    "\n",
    "avg_ridge_train_error_poly, avg_ridge_test_error_poly, = k_fold_cross_validation(10,X_2,y_2,10,\"ridge\")\n",
    "print(\"Average Ridge Train Error with polynomial features: \",avg_ridge_train_error_poly)\n",
    "print(\"Average Ridge Test Error with polynomial features: \",avg_ridge_test_error_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Summary and Evaluation</h2> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to best summarise my findings I have printed out the test error of all four models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Linear Test Error with no polynomial features:  23.636068605428157\n",
      "Average Ridge Test Error with no polynomial features:  23.68858300163877\n",
      "Average Linear Test Error with polynomial features:  11.854968236414866\n",
      "Average Ridge Test Error with polynomial features:  13.476138001136226\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Linear Test Error with no polynomial features: \",avg_linear_test_error_nopoly)\n",
    "print(\"Average Ridge Test Error with no polynomial features: \",avg_ridge_test_error_nopoly)\n",
    "print(\"Average Linear Test Error with polynomial features: \",avg_linear_test_error_poly)\n",
    "print(\"Average Ridge Test Error with polynomial features: \",avg_ridge_test_error_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of my findings is quite interesting and suprising. First let us compare the ridge regression\n",
    "vs linear regression. Usually one expects that because of the ridge regression, even though bias will go down, the \n",
    "error from variance will go down more and it will lead to a greater fall in error. However, this does not seem to \n",
    "be the case over here. In both instances (no polynomial features and polynomial features) the linear regression model\n",
    "outperforms the ridge regression model. Next let us see no polynomial features in dataset vs polynomial features in \n",
    "dataset. After introducing polynomial features, the test error in both the linear and ridge models went down \n",
    "significantly. The reason for this may have been that the features have more of a polynomial relation with the target \n",
    "variable as compared to a basic linear one. Introducing the polynomial features helped increase the complexity of the \n",
    "model, fit the dataset better, and hence reduce the test error. All in all then I would use the linear regression \n",
    "model with polynomial features to in the future to predict housing prices as it gave the least test error."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
